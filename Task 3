# Task 3 - Integrate the above jobs into a data pipeline, the job flow is important so step 2 should precede step 3 in the data pipeline 

Created the following datapipeline using google composer and apache airflow where the steps are given below:

1. Created a dataproc cluster 'create_dataproc_cluster'
2. Ran an ETL process as described in task 1. This calls etl_save.py.  'etl_process'
3. Ran 'dataproc_pyspark' job to show  descriptive summary and other computations. This calls the python script dataproc_nyctaxi.py. 
4. Deleted the dataproc cluster 'delete_dataproc_cluster'

The DAG  datapipeline.py is given below:

```
 from __future__ import print_function
 import datetime

from airflow import models
from airflow.operators import bash_operator
from airflow.operators import python_operator
from airflow.contrib.operators import dataproc_operator
from airflow.utils import trigger_rule
# from airflow.contrib.operators import bigquery_operator
from airflow.utils.dates import days_ago

yesterday = datetime.datetime.combine(
    datetime.datetime.today() - datetime.timedelta(1),
    datetime.datetime.min.time())


default_dag_args = {
    # Setting start date as yesterday starts the DAG immediately when it is
    # detected in the Cloud Storage bucket.
    'start_date': datetime.datetime(2021, 1, 5),
}

with models.DAG(
        'composer_datapipeline',
        schedule_interval=datetime.timedelta(days=1),
        default_args=default_dag_args) as dag:

    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(
       task_id='create_dataproc_cluster',
       project_id='dataproc-300110',
       cluster_name='cluster-58d6',
       num_workers=2,
       region='us-east1',
       master_machine_type='n1-standard-2',
       worker_machine_type='n1-standard-2'
    )

    etl_process = bash_operator.BashOperator(
        task_id='etl_process',
        # Run python file for ETL process.
        bash_command='python /home/airflow/gcs/data/etl_save.py')


    dataproc_pyspark = dataproc_operator.DataProcPySparkOperator(
        task_id = 'Query_Data_spark_job',
        #call the py file for processing
        main='gs://dataproc-nyc-taxi-2020/code_deploy/dataproc_nyctaxi.py',
        #main_jar='gs://dataproc-nyc-taxi-2020/code_deploy/myPy.jar',
        cluster_name='cluster-58d6',
        region='us-east1',
        dataproc_pyspark_jars=[ ]
    )

    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(
        project_id='dataproc-300110',
        task_id = 'delete_dataproc_cluster',
        cluster_name='cluster-58d6',
        region='us-east1',
        trigger_rule = trigger_rule.TriggerRule.ALL_DONE
    )

  create_dataproc_cluster >> etl_process >> dataproc_pyspark >> delete_dataproc_cluster
  ```
To see the output and screenshots of the above airflow datapipeline refer the Output section.
